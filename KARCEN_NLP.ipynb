{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50edd9c1-6f92-45b2-85af-f83170362e20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 323\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# #!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# # coding=utf-8\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# \"\"\"An optimized and streamlined implementation for sentiment analysis on the SST-5 dataset using DeBERTa variants.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m#     tf.keras.mixed_precision.set_global_policy(policy)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m#     main()\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m \u001b[38;5;28mprint\u001b[39m(history)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\"\"\"An optimized and streamlined implementation for sentiment analysis on the SST-5 dataset using DeBERTa variants.\n",
    "\n",
    "This code uses Hugging Face transformers and TensorFlow to train and evaluate DeBERTa models for sentiment classification.\n",
    "It includes data loading, preprocessing, model definitions, training, and evaluation with improved error handling.\n",
    "\n",
    "Author: Jiacheng Zheng\n",
    "Contact: karcenzheng@yeah.net\n",
    "Date: 2025/03/03\n",
    "Version: 1.2\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    TFDebertaV2Model\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['font.family'] = 'Garamond'\n",
    "\n",
    "# =====================\n",
    "# Data Loading Module\n",
    "# =====================\n",
    "class DataLoader:\n",
    "    @staticmethod\n",
    "    def load_sst5_data(data_dir=\"./SST5/\"):\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.makedirs(data_dir)\n",
    "            print(f\"Data directory created: {data_dir}\")\n",
    "\n",
    "        file_paths = {\n",
    "            'train': os.path.join(data_dir, 'train.jsonl'),\n",
    "            'dev': os.path.join(data_dir, 'dev.jsonl'),\n",
    "            'test': os.path.join(data_dir, 'test.jsonl')\n",
    "        }\n",
    "\n",
    "        for name, path in file_paths.items():\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"File {path} not found. Please ensure SST-5 dataset files are in {data_dir}\")\n",
    "\n",
    "        datasets = {}\n",
    "        for split in ['train', 'dev', 'test']:\n",
    "            texts, labels = [], []\n",
    "            with open(file_paths[split], 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    data = json.loads(line)\n",
    "                    texts.append(data['text'])\n",
    "                    labels.append(data['label'])\n",
    "            datasets[split] = (texts, labels)\n",
    "\n",
    "        train_texts, train_labels = datasets['train']\n",
    "        val_texts, val_labels = datasets['dev']\n",
    "        test_texts, test_labels = datasets['test']\n",
    "\n",
    "        print(f\"Dataset sizes - Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")\n",
    "        return train_texts, train_labels, val_texts, val_labels, test_texts, test_labels\n",
    "\n",
    "# =====================\n",
    "# Data Preprocessing Module\n",
    "# =====================\n",
    "class DataProcessor:\n",
    "    def __init__(self, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def create_tf_dataset(self, texts, labels, batch_size=16, shuffle=True):\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            {\n",
    "                'input_ids': encodings['input_ids'],\n",
    "                'attention_mask': encodings['attention_mask']\n",
    "            },\n",
    "            tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "        ))\n",
    "\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=len(texts), seed=SEED)\n",
    "        dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# =====================\n",
    "# Model Definition Module\n",
    "# =====================\n",
    "class SentimentModel(Model):\n",
    "    def __init__(self, num_labels=5):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def prepare_input(self, inputs):\n",
    "        return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "class TFDeBERTaForSentiment(Model):\n",
    "    def __init__(self, model_name=\"microsoft/deberta-v3-base\", num_labels=5):\n",
    "        super().__init__()\n",
    "        self.deberta = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            from_pt=True\n",
    "        )\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.deberta(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            training=training\n",
    "        )\n",
    "        return outputs.logits  # Return logits directly, apply softmax in loss function\n",
    "\n",
    "class TFDeBERTaLSTMModel(SentimentModel):\n",
    "    def __init__(self, model_name=\"microsoft/deberta-v3-base\", num_labels=5, dropout_rate=0.1):\n",
    "        super().__init__(num_labels)\n",
    "        self.deberta = TFDebertaV2Model.from_pretrained(model_name)\n",
    "        self.lstm = layers.LSTM(768, return_sequences=False)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.classifier = layers.Dense(num_labels)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, attention_mask = self.prepare_input(inputs)\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
    "        sequence_output = outputs[0]\n",
    "        lstm_output = self.lstm(sequence_output)\n",
    "        dropout_output = self.dropout(lstm_output, training=training)\n",
    "        return self.classifier(dropout_output)\n",
    "\n",
    "class TFDeBERTaFFNModel(SentimentModel):\n",
    "    def __init__(self, model_name=\"microsoft/deberta-v3-base\", num_labels=5, dropout_rate=0.1):\n",
    "        super().__init__(num_labels)\n",
    "        self.deberta = TFDebertaV2Model.from_pretrained(model_name)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.dense1 = layers.Dense(512, activation='relu')\n",
    "        self.dense2 = layers.Dense(256, activation='relu')\n",
    "        self.classifier = layers.Dense(num_labels)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, attention_mask = self.prepare_input(inputs)\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
    "        pooled_output = outputs[0][:, 0, :]  # CLS token\n",
    "        x = self.dropout(pooled_output, training=training)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class TFDeBERTaAdapterModel(SentimentModel):\n",
    "    def __init__(self, model_name=\"microsoft/deberta-v3-base\", num_labels=5, dropout_rate=0.1, bottleneck_dim=64):\n",
    "        super().__init__(num_labels)\n",
    "        self.deberta = TFDebertaV2Model.from_pretrained(model_name)\n",
    "        self.hidden_size = self.deberta.config.hidden_size\n",
    "        self.adapter_down = layers.Dense(bottleneck_dim, activation='relu')\n",
    "        self.adapter_up = layers.Dense(self.hidden_size)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.classifier = layers.Dense(num_labels)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, attention_mask = self.prepare_input(inputs)\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
    "        pooled_output = outputs[0][:, 0, :]\n",
    "        adapter_down = self.adapter_down(pooled_output)\n",
    "        adapter_up = self.adapter_up(adapter_down)\n",
    "        adapter_output = adapter_up + pooled_output\n",
    "        adapter_output = self.dropout(adapter_output, training=training)\n",
    "        return self.classifier(adapter_output)\n",
    "\n",
    "# =====================\n",
    "# Training and Evaluation Module\n",
    "# =====================\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, learning_rate=2e-5, weight_decay=0.01):\n",
    "        self.model = model\n",
    "        self.optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.metrics = [tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')]\n",
    "\n",
    "    def compile_model(self):\n",
    "        self.model.compile(optimizer=self.optimizer, loss=self.loss_fn, metrics=self.metrics)\n",
    "\n",
    "    def train(self, train_dataset, val_dataset, epochs=3):\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1)\n",
    "        ]\n",
    "        self.compile_model()\n",
    "        self.model.summary()\n",
    "        return self.model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    def evaluate(self, test_dataset):\n",
    "        self.compile_model()\n",
    "        test_results = self.model.evaluate(test_dataset, verbose=1, return_dict=True)\n",
    "        \n",
    "        y_true, y_pred = [], []\n",
    "        for batch_inputs, batch_labels in test_dataset:\n",
    "            preds = self.model(batch_inputs, training=False)\n",
    "            y_true.extend(batch_labels.numpy().tolist())\n",
    "            y_pred.extend(np.argmax(preds, axis=1).tolist())\n",
    "\n",
    "        report = classification_report(y_true, y_pred, target_names=[f\"Class {i}\" for i in range(5)], output_dict=True)\n",
    "        return {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision_macro': precision_score(y_true, y_pred, average='macro'),\n",
    "            'recall_macro': recall_score(y_true, y_pred, average='macro'),\n",
    "            'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "            'classification_report': report\n",
    "        }\n",
    "\n",
    "# =====================\n",
    "# Main Execution Module\n",
    "# =====================\n",
    "def main():\n",
    "    os.makedirs(\"./models/\", exist_ok=True)\n",
    "\n",
    "    print(\"Starting SST-5 sentiment analysis with DeBERTa...\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading dataset...\")\n",
    "    train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = DataLoader.load_sst5_data()\n",
    "\n",
    "    # Initialize tokenizer and processor\n",
    "    model_name = \"microsoft/deberta-v3-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    data_processor = DataProcessor(tokenizer, max_length=128)\n",
    "    batch_size = 16\n",
    "\n",
    "    # Create datasets\n",
    "    print(\"\\nCreating TensorFlow datasets...\")\n",
    "    train_dataset = data_processor.create_tf_dataset(train_texts, train_labels, batch_size=batch_size)\n",
    "    val_dataset = data_processor.create_tf_dataset(val_texts, val_labels, batch_size=batch_size, shuffle=False)\n",
    "    test_dataset = data_processor.create_tf_dataset(test_texts, test_labels, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # GPU configuration\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"\\n{len(gpus)} GPU(s) configured\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"\\nGPU config failed: {e}\")\n",
    "    else:\n",
    "        print(\"\\nNo GPUs detected. Running on CPU.\")\n",
    "\n",
    "    # Define models\n",
    "    models = {\n",
    "        \"DeBERTaForSentiment\": TFDeBERTaForSentiment(model_name=model_name),\n",
    "        \"DeBERTaLSTM\": TFDeBERTaLSTMModel(model_name=model_name),\n",
    "        \"DeBERTaFFN\": TFDeBERTaFFNModel(model_name=model_name),\n",
    "        \"DeBERTaAdapter\": TFDeBERTaAdapterModel(model_name=model_name)\n",
    "    }\n",
    "\n",
    "    # Train and evaluate\n",
    "    models_results = {}\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n{'='*50}\\nTraining {model_name} model...\\n{'='*50}\")\n",
    "        \n",
    "        if hasattr(model, 'deberta') and not isinstance(model, TFDeBERTaForSentiment):\n",
    "            model.deberta.trainable = False\n",
    "            print(f\"Base DeBERTa frozen for {model_name}\")\n",
    "\n",
    "        trainer = ModelTrainer(model)\n",
    "        try:\n",
    "            history = trainer.train(train_dataset, val_dataset, epochs=3)\n",
    "            print(f\"\\nEvaluating {model_name} model...\")\n",
    "            results = trainer.evaluate(test_dataset)\n",
    "            models_results[model_name] = results\n",
    "            model.save_weights(f\"./models/{model_name}_weights.h5\")\n",
    "            print(f\"{model_name} model weights saved\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name}: {str(e)}\")\n",
    "            print(\"Skipping to next model\")\n",
    "            continue\n",
    "\n",
    "    # Print results\n",
    "    if models_results:\n",
    "        print(\"\\nModel Performance Summary:\")\n",
    "        for name, results in models_results.items():\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "            print(f\"  Precision: {results['precision_macro']:.4f}\")\n",
    "            print(f\"  Recall: {results['recall_macro']:.4f}\")\n",
    "            print(f\"  F1 Score: {results['f1_macro']:.4f}\")\n",
    "\n",
    "    print(\"\\nAll done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use mixed precision for faster training\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a537c69-75fd-44fd-a791-9a2609703256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.49.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)  # 应输出4.x.x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
